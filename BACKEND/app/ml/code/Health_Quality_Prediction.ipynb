{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FTyakkbz4tEb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ------------------------\n",
        "# 1. Load dataset\n",
        "# ------------------------\n",
        "df = pd.read_csv(\"Health_Quality_Dataset.csv\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Features & Target\n",
        "# ------------------------\n",
        "X = df.drop(columns=[\"true_prob\"])\n",
        "y = df[\"true_prob\"]\n",
        "\n",
        "# Identify categorical & numeric columns\n",
        "categorical_cols = [\"sex\", \"sanitation\", \"water_source\"]\n",
        "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
        "\n",
        "# ------------------------\n",
        "# 3. Preprocessing: One-hot encode categorical vars\n",
        "# ------------------------\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 4. Train/Test Split\n",
        "# ------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------\n",
        "# 5. Build Random Forest Pipeline\n",
        "# ------------------------\n",
        "rf_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"regressor\", RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=12,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "# Fit model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# 6. Evaluation\n",
        "# ------------------------\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "# Calculate RMSE by taking the square root of the MSE\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# ------------------------\n",
        "# 7. Feature Importance (approx)\n",
        "# ------------------------\n",
        "# To get feature importances, extract from the trained regressor\n",
        "regressor = rf_model.named_steps[\"regressor\"]\n",
        "\n",
        "# Get one-hot encoded feature names\n",
        "ohe = rf_model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"]\n",
        "ohe_features = ohe.get_feature_names_out(categorical_cols)\n",
        "all_features = np.concatenate([ohe_features, numeric_cols])\n",
        "\n",
        "importances = regressor.feature_importances_\n",
        "feat_imp = pd.DataFrame({\"feature\": all_features, \"importance\": importances})\n",
        "feat_imp = feat_imp.sort_values(\"importance\", ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Important Features:\")\n",
        "print(feat_imp.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDSfpuso98hJ",
        "outputId": "44fc3985-8296-4698-bb96-4ce25069e8d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score: 0.8628063947863092\n",
            "MAE: 0.031479130729042\n",
            "RMSE: 0.04044361076109148\n",
            "\n",
            "Top 10 Important Features:\n",
            "            feature  importance\n",
            "17         vomiting    0.234174\n",
            "16         diarrhea    0.227126\n",
            "20         jaundice    0.200509\n",
            "18            fever    0.156700\n",
            "15              age    0.028468\n",
            "2   sanitation_good    0.021017\n",
            "19          fatigue    0.017904\n",
            "3   sanitation_poor    0.017791\n",
            "22    loss_appetite    0.013784\n",
            "23     muscle_aches    0.011096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"regressor\", XGBRegressor(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"XGBoost R²:\", r2_score(y_test, y_pred_xgb))\n",
        "print(\"XGBoost MAE:\", mean_absolute_error(y_test, y_pred_xgb))\n",
        "print(\"XGBoost RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0YgBFGG5GBh",
        "outputId": "2c280f8d-e8c3-47f8-cc9a-c1a956803363"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost R²: 0.8791551226249125\n",
            "XGBoost MAE: 0.029363521593616018\n",
            "XGBoost RMSE: 0.03795745713675475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "lgb_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"regressor\", LGBMRegressor(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=-1,\n",
        "        num_leaves=31,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "\n",
        "print(\"LightGBM R²:\", r2_score(y_test, y_pred_lgb))\n",
        "print(\"LightGBM MAE:\", mean_absolute_error(y_test, y_pred_lgb))\n",
        "print(\"LightGBM RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_lgb)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9YnGYhx6eou",
        "outputId": "100a8eb6-2d9a-4a1d-da51-62f74f409328"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 128\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 0.767528\n",
            "LightGBM R²: 0.8820813995754307\n",
            "LightGBM MAE: 0.02901926615722816\n",
            "LightGBM RMSE: 0.037495067999284884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "et_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"regressor\", ExtraTreesRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "et_model.fit(X_train, y_train)\n",
        "y_pred_et = et_model.predict(X_test)\n",
        "\n",
        "print(\"ExtraTrees R²:\", r2_score(y_test, y_pred_et))\n",
        "print(\"ExtraTrees MAE:\", mean_absolute_error(y_test, y_pred_et))\n",
        "print(\"ExtraTrees RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_et)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rgui0Go7KJv",
        "outputId": "24509a3f-c8aa-483c-927e-1c81dad1d3a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExtraTrees R²: 0.8007507451317124\n",
            "ExtraTrees MAE: 0.037524568792806676\n",
            "ExtraTrees RMSE: 0.04873953040910984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save Random Forest\n",
        "joblib.dump(rf_model, \"rf_health_model.pkl\")\n",
        "\n",
        "# Save XGBoost\n",
        "joblib.dump(xgb_model, \"xgb_health_model.pkl\")\n",
        "\n",
        "# Save LightGBM\n",
        "joblib.dump(lgb_model, \"lgb_health_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6zkW9a27QW1",
        "outputId": "e79eebb9-0510-4815-9d77-d2e11aa25840"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lgb_health_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2x0nPatt8AVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}